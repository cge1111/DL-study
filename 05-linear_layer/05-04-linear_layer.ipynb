{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Linear Layer\n",
    "\n",
    "- 신경망의 가장 기본 구성 요소\n",
    "- fully-connected(FC) Layer라고 불리기도 함\n",
    "- 내부 파라미터에 따른 선형 변환을 수행하는 함수\n",
    "- **각 입력 노드들에 weight(가중치)를 곱하고 모두 합친 뒤, bias(편향)을 더한다.**\n",
    "- 행렬 곱으로 구현 가능\n",
    "- n차원에서 m차원으로의 ``선형 변환 함수``\n",
    "\n",
    "$$\\begin{gathered}\n",
    "y=x\\cdot{W}+b, \\\\\n",
    "\\text{where }x\\in\\mathbb{R}^{N\\times{n}}\\text{, }y\\in\\mathbb{R}^{N\\times{m}}. \\\\\n",
    "\\\\\n",
    "\\text{Thus, }W\\in\\mathbb{R}^{n\\times{m}}\\text{ and }b\\in\\mathbb{R}^m.\n",
    "\\end{gathered}$$\n",
    "\n",
    "- 내부 가중치 파라미터 W와 b에 의해 정의됨\n",
    "- 함수의 파라미터를 잘 조절하면, 주어진 입력에 대해 원하는 출력을 만들 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.FloatTensor([[1, 2],\n",
    "                       [3, 4],\n",
    "                       [5, 6]])\n",
    "b = torch.FloatTensor([2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(W.size())\n",
    "print(b.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(x, W, b):\n",
    "    y = torch.matmul(x, W) + b\n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor([[1, 1, 1],\n",
    "                       [2, 2, 2],\n",
    "                       [3, 3, 3],\n",
    "                       [4, 4, 4]])\n",
    "\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = linear(x, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = torch.FloatTensor(input_dim, output_dim)\n",
    "        self.b = torch.FloatTensor(output_dim)\n",
    "\n",
    "    # You should override 'forward' method to implement detail.\n",
    "    # The input arguments and outputs can be designed as you wish.\n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "        #     = (batch_size, output_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 틀린 방법이라 답 안나옴\n",
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that there is no weight parameters to learn.\n",
    "Above way can forward(or calculate) values, but it cannot be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct way: nn.Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = nn.Parameter(torch.FloatTensor(input_dim, output_dim)) # 제대로 등록하려면 nn.params 등록\n",
    "        self.b = nn.Parameter(torch.FloatTensor(output_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = torch.matmul(x, self.W) + self.b\n",
    "        # |y| = (batch_size, input_dim) * (input_dim, output_dim)\n",
    "        #     = (batch_size, output_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/docs/stable/nn.html#torch.nn.Parameter\n",
    "\n",
    "A kind of Tensor that is to be considered a module parameter.\n",
    "\n",
    "Parameters are Tensor subclasses, that have a very special property when used with Module s - when they’re assigned as Module attributes they are automatically added to the list of its parameters, and will appear e.g. in parameters() iterator. Assigning a Tensor doesn’t have such effect. This is because one might want to cache some temporary state, like last hidden state of the RNN, in the model. If there was no such class as Parameter, these temporaries would get registered too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "\n",
    "y = linear(x) # 매핑하지 않아도 forward를 부름(파이토치가 해줌)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000e+00, -0.0000e+00],\n",
      "        [ 9.1002e+31, -2.8586e-42],\n",
      "        [ 8.4078e-45,  0.0000e+00]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([4.7428e+30, 7.1429e+31], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Linear (선형 변환 함수)\n",
    "클래스 구현 안해도 nn사용해서 불러올 수 있다! (내장함수 nn 이용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(3, 2)\n",
    "\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3386, -0.5355, -0.4991],\n",
      "        [ 0.1993,  0.4776,  0.1894]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1819, 0.0941], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nn.Module can contain other nn.Module's child classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim=3, output_dim=2):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # |x| = (batch_size, input_dim)\n",
    "        y = self.linear(x)\n",
    "        # |y| = (batch_size, output_dim)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = MyLinear(3, 2)\n",
    "\n",
    "y = linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2])\n"
     ]
    }
   ],
   "source": [
    "print(y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1267,  0.0563,  0.3951],\n",
      "        [ 0.2291,  0.3214,  0.2595]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.3659, 0.4013], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in linear.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
